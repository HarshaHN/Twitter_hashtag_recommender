{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 1 completed in  7.014540672302246\n"
     ]
    }
   ],
   "source": [
    "#Data Cleansing part 1\n",
    "\"\"\"\n",
    "Created on Sun Dec 16 19:12:02 2018\n",
    "@author: HN\n",
    "\"\"\"\n",
    "import codecs\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "#import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from sklearn.metrics import precision_recall_fscore_support as score\n",
    "#from spacy.lemmatizer import Lemmatizer as lemma\n",
    "#from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "#from nltk.tokenize import word_tokenize\n",
    "start = time.time();\n",
    "\n",
    "script_dir = os.path.dirname(os.path.realpath('__file__')) #<-- absolute dir the script is in\n",
    "rel_path = \"02_data/\" ; abs_file_path = os.path.join(script_dir, rel_path)\n",
    "\n",
    "Files = [\"wallstreet.csv\", \"winter.csv\", \"christmas.csv\", \"britishcricket.csv\", \"scubadive.csv\", \"brexit.csv\" ];\n",
    "out = [\"allwallstreet.txt\",\"allwinter.txt\", \"allchristmas.txt\", \"allbritishcricket.txt\", \"allscubadive.txt\", \"allbrexit.txt\" ]; ind = -1;\n",
    "\n",
    "cleaned_tweets = defaultdict(list); all_hashtags = defaultdict(list);\n",
    "stop_words = set(stopwords.words('english')); \n",
    "\n",
    "for file in Files: \n",
    "    ind += 1\n",
    "    outfile = os.path.join(abs_file_path, out[ind])\n",
    "    with codecs.open(outfile, \"w\", \"utf-8\") as out_data:\n",
    "        \n",
    "        readfile = os.path.join(abs_file_path, file)\n",
    "        #print(readfile)\n",
    "        with open(readfile, \"r\", encoding = \"utf8\") as my_input_file:\n",
    "            \n",
    "            for line in my_input_file:\n",
    "                # clean data\n",
    "                line = re.sub(\"`|@|,|~|\\n|'['|']'|'?'|<.*>|<|!|\\.|\\$|\\*|:|%|\\+|…|\\\\\\\\|\\/|«|»|···|\\||\\•|\\?|\\(|\\)|=|-|&|;|\\_|—|~|¯|\\{|\\}|\\[|\\]|£|€|¥|¿|–|\\“|\\”|\\‘|\\’|\\\"|,|'\", \" \", line)\n",
    "                line = re.sub(\"http[a-zA-Z0-9]+\", \"\", line)\n",
    "                line = re.sub(\"pictwitter\",\" pictwitter\", line); line = re.sub(\" # \",\" #\", line); line = re.sub(\"#\",\" #\", line); line = re.sub(\"# \",\"\", line); \n",
    "                line = line.lower(); \n",
    "\n",
    "                # remove stop words\n",
    "                word_list = line.split(' ');  filt_words = []\n",
    "                for word in word_list:\n",
    "                    if ((word not in stop_words) and (word != '' and len(word)>2) and (not word.__contains__('pictwitter'))):\n",
    "                        filt_words.append(re.sub(\"  \",\" \", word)); \n",
    "                        if (word[0] == '#') and (len(word)> 1) : all_hashtags[ind].append(word); \n",
    "\n",
    "                filt_tweet = ' '.join(filt_words); \n",
    "                cleaned_tweets[ind].append(filt_tweet)\n",
    "                out_data.write(filt_tweet+'\\n')\n",
    "\n",
    "print(\"Checkpoint 1 completed in \", time.time() - start);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 2 completed in  0.8285253047943115\n"
     ]
    }
   ],
   "source": [
    "#Data Cleansing part 2\n",
    "start = time.time();\n",
    "top_hashtags = []\n",
    "\n",
    "for tag in all_hashtags.keys():\n",
    "    hashtag = FreqDist(all_hashtags[tag]).most_common(1)[0][0]\n",
    "    top_hashtags.append(hashtag)\n",
    "    \n",
    "selectedtweets = [\"wallstreet.txt\",\"winter.txt\", \"christmas.txt\", \"cricket.txt\", \"scubadive.txt\", \"brexit.txt\" ]; ind = -1;\n",
    "hashtag_file = os.path.join(abs_file_path, \"classes.txt\")\n",
    "\n",
    "#Top hashtags in each catagory\n",
    "with codecs.open(hashtag_file, \"w\", \"utf-8\") as hashfile:\n",
    "    for tag in top_hashtags:\n",
    "        hashfile.write(tag + '\\n')\n",
    "\n",
    "for file in out:\n",
    "    ind += 1;\n",
    "    \n",
    "    readfile = os.path.join(abs_file_path, out[ind])\n",
    "    with codecs.open(readfile, \"r\", \"utf-8\") as read_file:\n",
    "        \n",
    "        writefile = os.path.join(abs_file_path, selectedtweets[ind])\n",
    "        with codecs.open(writefile, \"w\", \"utf-8\") as write_file:\n",
    "            for line in read_file:\n",
    "                if any(s in line for s in top_hashtags):\n",
    "                    write_file.write(line)\n",
    "\n",
    "print(\"Checkpoint 2 completed in \", time.time() - start);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 3 completed in  87.499755859375\n"
     ]
    }
   ],
   "source": [
    "#Data segregation\n",
    "start = time.time();\n",
    "\n",
    "tweets = selectedtweets; hashtags = top_hashtags;\n",
    "\n",
    "# Dict = {Hashtag: Tweets} and bag of words\n",
    "HT = dict.fromkeys(hashtags,''); \n",
    "trainHT = defaultdict(list);  testHT = defaultdict(list);\n",
    "ind = -1; bow = set(); #hashtweets = defaultdict(list);\n",
    "\n",
    "for file in tweets: \n",
    "    \n",
    "    ind += 1\n",
    "    infile = os.path.join(abs_file_path, file)\n",
    "    with codecs.open(infile, \"r\", \"utf-8\") as in_data:\n",
    "        for line in in_data:\n",
    "            \n",
    "            for tag in hashtags:\n",
    "                if tag in line:\n",
    "                    flag = np.random.choice([0, 1], 1, p=[0.75, 0.25])\n",
    "                    if flag == 1:\n",
    "                        #Test data: Dict = {Hashtag: Tweets}\n",
    "                        testHT[tag].append(re.sub('#' , '', line))\n",
    "                    else: \n",
    "                        # Train set: Dict = {Hashtag: Tweets}\n",
    "                        trainHT[tag].append(re.sub('#' , '', line)) # {Hashtag: [T1, T2, T3]}\n",
    "                        HT[tag] += re.sub('#' , '', line) # {Hashtag: ['T1 T2 T3']}\n",
    "\n",
    "print(\"Checkpoint 3 completed in \", time.time() - start);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     0            1            2             3            4   \\\n",
      "#wallstreet  wallstreet   joinupdots          nyc     dreamlife  newyorkcity   \n",
      "#winter       transfers    minibuses      coaches       shuttle         cabs   \n",
      "#christmas    christmas       pastry  thecakeshop    pastrychef        pembs   \n",
      "#cricket        cricket        f7fy2       string  constitution          dld   \n",
      "#scubadive    scubadive  scubadiving        scuba          dive   scubadiver   \n",
      "#brexit      referendum  peoplesvote       labour     democracy         tory   \n",
      "\n",
      "                          5                      6               7   \\\n",
      "#wallstreet        manhattan                 banker           quits   \n",
      "#winter      heathrowshuttle               columbia      londonlife   \n",
      "#christmas          cakeshop               pembroke           cakes   \n",
      "#cricket               moeen                  sport          asians   \n",
      "#scubadive         scubalife  underwaterphotography      underwater   \n",
      "#brexit                  mps                 tories  brexitshambles   \n",
      "\n",
      "                              8                     9      ...      \\\n",
      "#wallstreet              tweet10         success\\nwall     ...       \n",
      "#winter              visitlondon  falconbritishnursery     ...       \n",
      "#christmas   thecakeshoppembroke           buttericing     ...       \n",
      "#cricket                   balls                 match     ...       \n",
      "#scubadive                 diver              scubapro     ...       \n",
      "#brexit                    blair              uklabour     ...       \n",
      "\n",
      "                            15                       16             17  \\\n",
      "#wallstreet             stocks                      207      passenger   \n",
      "#winter                  vikes                   canada        britain   \n",
      "#christmas   greatbritishchefs                   hamper       columbia   \n",
      "#cricket              atherton  xr9y30dwxok\\njonleelife  cricket\\nlike   \n",
      "#scubadive    greatbarrierreef                    gopro      indonesia   \n",
      "#brexit                theresa              brexitchaos     government   \n",
      "\n",
      "                            18             19           20              21  \\\n",
      "#wallstreet               nyse             ny      finance     centralpark   \n",
      "#winter                     bc      vancouver  countryside  winteriscoming   \n",
      "#christmas              folksy  christmastime        icing     buttercream   \n",
      "#cricket     engvind\\nwatching         adrian     pakistan            bcci   \n",
      "#scubadive             sealife           reef       divers          turtle   \n",
      "#brexit                   vote      political      skynews            ukip   \n",
      "\n",
      "                        22              23          24  \n",
      "#wallstreet          finds           promo     trading  \n",
      "#winter             teamgb   winterconcert    skeleton  \n",
      "#christmas   christmastree  christmasgifts   vancouver  \n",
      "#cricket            akhtar         blofeld         t20  \n",
      "#scubadive       sdidivers          paditv  freediving  \n",
      "#brexit               tony      theresamay          eu  \n",
      "\n",
      "[6 rows x 25 columns]\n",
      "Checkpoint 4 completed in  1.68802809715271\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#TF-IDF Section\n",
    "start = time.time();                    \n",
    "\n",
    "#Bag of words\n",
    "#1 creation\n",
    "for txt in HT.values():\n",
    "    bow = set(bow).union(set(txt.split(' ')))\n",
    "#2 Bag of words initialization \n",
    "bowDict = {}\n",
    "for h in HT.keys():\n",
    "    bowDict[h] = dict.fromkeys(bow, 0)\n",
    "    #3 Bow updation for each class(hashtag)\n",
    "    for w in HT[h].split(' ') :\n",
    "        bowDict[h][w] += 1\n",
    "\n",
    "#Compute Term-Frequency\n",
    "def computeTF(bowDict, tweet):\n",
    "    tfDict = {}\n",
    "    tweetlen = len(tweet.split(' '))\n",
    "    for word, count in bowDict.items():\n",
    "        tfDict[word] = count/float(tweetlen)\n",
    "    return tfDict\n",
    "\n",
    "tf = {}\n",
    "for h, t in HT.items():\n",
    "    tf[h] = computeTF(bowDict[h], t)\n",
    "\n",
    "#Compute Inverse document frequency\n",
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    idfDict = HT.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList: #Each class(hashtag)\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                if word not in idfDict.keys():\n",
    "                    idfDict[word] = 1\n",
    "                else :\n",
    "                    idfDict[word] += 1\n",
    "\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "\n",
    "    return idfDict\n",
    "\n",
    "idfs = computeIDF([bowDict[h] for h in bowDict.keys()])\n",
    "\n",
    "#Compute TF-IDF\n",
    "def computeTFIDF(tfhashtxt, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfhashtxt.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "tfidfBow = {}; topfivewords = {}\n",
    "for hashtag in HT.keys():\n",
    "    tfidfBow[hashtag] = computeTFIDF(tf[hashtag], idfs)\n",
    "    topfivewords[hashtag] = dict(Counter(tfidfBow[hashtag]).most_common(25))\n",
    "\n",
    "tfidf = {}\n",
    "for hashtag, rankwords in topfivewords.items():\n",
    "    tfidf[hashtag] = rankwords.keys()\n",
    "        \n",
    "print(pd.DataFrame.from_dict(tfidf, orient='index'))\n",
    "\n",
    "print(\"Checkpoint 4 completed in \", time.time() - start);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data created 12.351907730102539\n",
      "Test data created 4.350520610809326\n",
      "Checkpoint 5 completed in  18.01665472984314\n",
      "50 Success!!!\n"
     ]
    }
   ],
   "source": [
    "#Vectors set\n",
    "start = time.time();         \n",
    "\n",
    "\"\"\"To transform train and test dataset into input and output vectors \n",
    "for the multiclass-logistic regression.\"\"\"\n",
    "\n",
    "rank = tfidfpd; \n",
    "trainXY = pd.DataFrame.from_dict(trainHT, orient='index')\n",
    "testXY = pd.DataFrame.from_dict(testHT, orient='index')\n",
    "\n",
    "bow = set()\n",
    "for i in range(0, len(rank)):\n",
    "    rankwords = rank.iloc[i]\n",
    "    bow = set(bow).union(set(rankwords))\n",
    "\n",
    "#Train data X Y\n",
    "start_time = time.time()\n",
    "X = list(bow); Y = list(rank.index.get_values());\n",
    "r, c = trainXY.shape;\n",
    "train = {}; train[tuple([0]*len(X))] = 0;\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(r):\n",
    "    outY = i\n",
    "\n",
    "    for j in range(c):\n",
    "        if trainXY.iloc[i, j] != None :\n",
    "            inX = [0]*len(X)\n",
    "\n",
    "            for feature in X:\n",
    "                if feature in trainXY.iloc[i, j]:\n",
    "                    inX[X.index(feature)] = 1\n",
    "                    if tuple(inX) not in train:\n",
    "                        train[tuple(inX)] = outY\n",
    "        else : \n",
    "            break    \n",
    "\n",
    "trainpd = pd.DataFrame.from_dict(train, orient='index')\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print('Train data created',elapsed_time)\n",
    "\n",
    "#Test data X Y\n",
    "X = list(bow); Y = list(rank.index.get_values());\n",
    "r, c = testXY.shape;\n",
    "test = {}; test[tuple([0]*len(X))] = 0;\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(r):\n",
    "    outY = i\n",
    "\n",
    "    for j in range(c):\n",
    "        if testXY.iloc[i, j] != None :\n",
    "            inX = [0]*len(X)\n",
    "\n",
    "            for feature in X:\n",
    "                if feature in testXY.iloc[i, j]:\n",
    "                    inX[X.index(feature)] = 1\n",
    "                    if tuple(inX) not in test:\n",
    "                        test[tuple(inX)] = outY\n",
    "        else : \n",
    "            break  \n",
    "                \n",
    "testpd = pd.DataFrame.from_dict(test, orient='index')\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print('Test data created',elapsed_time)       \n",
    "\n",
    "print(\"Checkpoint 5 completed in \", time.time() - start); print(\"Vector set created --> Success!!!\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22  1  0  0  0  0]\n",
      " [ 1 16  0  0  1  1]\n",
      " [ 0  1  5  1  0  0]\n",
      " [ 0  1  0 27  0  0]\n",
      " [ 0  0  0  2  6  0]\n",
      " [ 0  0  0  1  0 18]]\n",
      "Accuracy:  0.9038461538461539\n",
      "precision: [0.95652174 0.84210526 1.         0.87096774 0.85714286 0.94736842]\n",
      "recall: [0.95652174 0.84210526 0.71428571 0.96428571 0.75       0.94736842]\n",
      "fscore: [0.95652174 0.84210526 0.83333333 0.91525424 0.8        0.94736842]\n",
      "support: [23 19  7 28  8 19]\n",
      "Checkpoint 6 completed in  0.18689417839050293\n",
      "Task complete -- > Success!!!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "#Logistic regression\n",
    "start = time.time();  \n",
    "\n",
    "#Train data\n",
    "train = trainpd;\n",
    "trainX = np.array(list(train.index.get_values()))\n",
    "trainY = []\n",
    "for j in range(len(train)):\n",
    "    trainY.append(list(train.iloc[j])[0])\n",
    "trainY = np.array(trainY)\n",
    "\n",
    "#Test data\n",
    "test = testpd;\n",
    "testX = np.array(list(test.index.get_values()))\n",
    "testY = []\n",
    "for j in range(len(test)):\n",
    "    testY.append(list(test.iloc[j])[0])\n",
    "testY = np.array(testY)\n",
    "\n",
    "#Logistic regression.\n",
    "logreg = LogisticRegression( C=1e1, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg.fit(trainX, trainY)\n",
    "PredY = logreg.predict(testX)\n",
    "\n",
    "print(confusion_matrix(testY, PredY))\n",
    "\n",
    "accuracy = accuracy_score(testY, PredY)\n",
    "print('Accuracy: ', accuracy)\n",
    "\n",
    "precision, recall, fscore, support = score(testY, PredY)\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "\n",
    "print(\"Checkpoint 6 completed in \", time.time() - start);  print(\"Task complete -- > Success!!!\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
