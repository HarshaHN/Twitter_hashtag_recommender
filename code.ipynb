{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 1 completed in  8.368194103240967\n"
     ]
    }
   ],
   "source": [
    "#Data Cleansing part 1\n",
    "\"\"\"\n",
    "Created on Sun Dec 16 19:12:02 2018\n",
    "@author: HN\n",
    "\"\"\"\n",
    "import codecs\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "#import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "#from spacy.lemmatizer import Lemmatizer as lemma\n",
    "#from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "#from nltk.tokenize import word_tokenize\n",
    "start = time.time();\n",
    "\n",
    "script_dir = os.path.dirname(os.path.realpath('__file__')) #<-- absolute dir the script is in\n",
    "rel_path = \"02_data/\" ; abs_file_path = os.path.join(script_dir, rel_path)\n",
    "\n",
    "Files = [\"wallstreet.csv\", \"winter.csv\", \"christmas.csv\", \"britishcricket.csv\", \"scubadive.csv\", \"brexit.csv\" ];\n",
    "out = [\"allwallstreet.txt\",\"allwinter.txt\", \"allchristmas.txt\", \"allbritishcricket.txt\", \"allscubadive.txt\", \"allbrexit.txt\" ]; ind = -1;\n",
    "\n",
    "cleaned_tweets = defaultdict(list); all_hashtags = defaultdict(list);\n",
    "stop_words = set(stopwords.words('english')); \n",
    "\n",
    "for file in Files: \n",
    "    ind += 1\n",
    "    outfile = os.path.join(abs_file_path, out[ind])\n",
    "    with codecs.open(outfile, \"w\", \"utf-8\") as out_data:\n",
    "        \n",
    "        readfile = os.path.join(abs_file_path, file)\n",
    "        #print(readfile)\n",
    "        with open(readfile, \"r\", encoding = \"utf8\") as my_input_file:\n",
    "            \n",
    "            for line in my_input_file:\n",
    "                # clean data\n",
    "                line = re.sub(\"`|@|,|~|\\n|'['|']'|'?'|<.*>|<|!|\\.|\\$|\\*|:|%|\\+|…|\\\\\\\\|\\/|«|»|···|\\||\\•|\\?|\\(|\\)|=|-|&|;|\\_|—|~|¯|\\{|\\}|\\[|\\]|£|€|¥|¿|–|\\“|\\”|\\‘|\\’|\\\"|,|'\", \" \", line)\n",
    "                line = re.sub(\"http[a-zA-Z0-9]+\", \"\", line)\n",
    "                line = re.sub(\"pictwitter\",\" pictwitter\", line); line = re.sub(\" # \",\" #\", line); line = re.sub(\"#\",\" #\", line); line = re.sub(\"# \",\"\", line); \n",
    "                line = line.lower(); \n",
    "\n",
    "                # remove stop words\n",
    "                word_list = line.split(' ');  filt_words = []\n",
    "                for word in word_list:\n",
    "                    if ((word not in stop_words) and (word != '' and len(word)>2) and (not word.__contains__('pictwitter'))):\n",
    "                        filt_words.append(re.sub(\"  \",\" \", word)); \n",
    "                        if (word[0] == '#') and (len(word)> 1) : all_hashtags[ind].append(word); \n",
    "\n",
    "                filt_tweet = ' '.join(filt_words); \n",
    "                cleaned_tweets[ind].append(filt_tweet)\n",
    "                out_data.write(filt_tweet+'\\n')\n",
    "\n",
    "print(\"Checkpoint 1 completed in \", time.time() - start);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 2 completed in  1.07438325881958\n"
     ]
    }
   ],
   "source": [
    "#Data Cleansing part 2\n",
    "start = time.time();\n",
    "top_hashtags = []\n",
    "\n",
    "for tag in all_hashtags.keys():\n",
    "    hashtag = FreqDist(all_hashtags[tag]).most_common(1)[0][0]\n",
    "    top_hashtags.append(hashtag)\n",
    "    \n",
    "selectedtweets = [\"wallstreet.txt\",\"winter.txt\", \"christmas.txt\", \"cricket.txt\", \"scubadive.txt\", \"brexit.txt\" ]; ind = -1;\n",
    "hashtag_file = os.path.join(abs_file_path, \"classes.txt\")\n",
    "\n",
    "#Top hashtags in each catagory\n",
    "with codecs.open(hashtag_file, \"w\", \"utf-8\") as hashfile:\n",
    "    for tag in top_hashtags:\n",
    "        hashfile.write(tag + '\\n')\n",
    "\n",
    "for file in out:\n",
    "    ind += 1;\n",
    "    \n",
    "    readfile = os.path.join(abs_file_path, out[ind])\n",
    "    with codecs.open(readfile, \"r\", \"utf-8\") as read_file:\n",
    "        \n",
    "        writefile = os.path.join(abs_file_path, selectedtweets[ind])\n",
    "        with codecs.open(writefile, \"w\", \"utf-8\") as write_file:\n",
    "            for line in read_file:\n",
    "                if any(s in line for s in top_hashtags):\n",
    "                    write_file.write(line)\n",
    "\n",
    "print(\"Checkpoint 2 completed in \", time.time() - start);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 3 completed in  107.3873348236084\n"
     ]
    }
   ],
   "source": [
    "#Data segregation\n",
    "start = time.time();\n",
    "\n",
    "tweets = selectedtweets; hashtags = top_hashtags;\n",
    "\n",
    "# Dict = {Hashtag: Tweets} and bag of words\n",
    "HT = dict.fromkeys(hashtags,''); \n",
    "trainHT = defaultdict(list);  testHT = defaultdict(list);\n",
    "ind = -1; bow = set(); #hashtweets = defaultdict(list);\n",
    "\n",
    "for file in tweets: \n",
    "    \n",
    "    ind += 1\n",
    "    infile = os.path.join(abs_file_path, file)\n",
    "    with codecs.open(infile, \"r\", \"utf-8\") as in_data:\n",
    "        for line in in_data:\n",
    "            \n",
    "            for tag in hashtags:\n",
    "                if tag in line:\n",
    "                    flag = np.random.choice([0, 1], 1, p=[0.75, 0.25])\n",
    "                    if flag == 1:\n",
    "                        #Test data: Dict = {Hashtag: Tweets}\n",
    "                        testHT[tag].append(re.sub('#' , '', line))\n",
    "                    else: \n",
    "                        # Train set: Dict = {Hashtag: Tweets}\n",
    "                        trainHT[tag].append(re.sub('#' , '', line)) # {Hashtag: [T1, T2, T3]}\n",
    "                        HT[tag] += re.sub('#' , '', line) # {Hashtag: ['T1 T2 T3']}\n",
    "\n",
    "print(\"Checkpoint 3 completed in \", time.time() - start);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        0           1                2           3   \\\n",
      "#wallstreet     wallstreet  joinupdots              nyc   dreamlife   \n",
      "#winter          transfers   minibuses  heathrowshuttle     coaches   \n",
      "#christmas   christmastree      pastry      thecakeshop  pastrychef   \n",
      "#cricket             f7fy2     cricket           string         dld   \n",
      "#scubadive           scuba   scubadive      scubadiving        dive   \n",
      "#brexit        peoplesvote  referendum           labour      corbyn   \n",
      "\n",
      "                       4           5                     6   \\\n",
      "#wallstreet   newyorkcity   manhattan                banker   \n",
      "#winter              cabs     shuttle  falconbritishnursery   \n",
      "#christmas       pembroke       pembs         pembrokeshire   \n",
      "#cricket     constitution       moeen                asians   \n",
      "#scubadive     scubadiver  underwater             scubalife   \n",
      "#brexit         democracy        tory                   mps   \n",
      "\n",
      "                                7                    8               9   \\\n",
      "#wallstreet                  quits              tweet10   success\\nwall   \n",
      "#winter                   columbia           londoncity  winterolympics   \n",
      "#christmas                cakeshop  thecakeshoppembroke      spongecake   \n",
      "#cricket                     sport       grown\\ncricket     grown\\nlike   \n",
      "#scubadive   underwaterphotography             scubapro            padi   \n",
      "#brexit                     tories       brexitshambles        uklabour   \n",
      "\n",
      "                   ...                 15                 16             17  \\\n",
      "#wallstreet        ...             stocks                207      passenger   \n",
      "#winter            ...          vancouver            houston         teamgb   \n",
      "#christmas         ...              cakes           columbia          icing   \n",
      "#cricket           ...             akhtar  engvind\\nwatching  cricket\\nlike   \n",
      "#scubadive         ...              diver              gopro          buceo   \n",
      "#brexit            ...                wto        brexitchaos        theresa   \n",
      "\n",
      "                                  18         19                   20  \\\n",
      "#wallstreet                     nyse    finance                   ny   \n",
      "#winter                winterconcert   heathrow  ukaccreditednursery   \n",
      "#christmas               buttercream  vancouver                mince   \n",
      "#cricket     xr9y30dwxok\\njonleelife      match            suspicion   \n",
      "#scubadive                   sealife     divers               turtle   \n",
      "#brexit                         deal       vote              cabinet   \n",
      "\n",
      "                       21                  22        23                 24  \n",
      "#wallstreet  breakingnews                nyti     finds        centralpark  \n",
      "#winter        inabudhabi  winterolympics2018  skeleton        countryside  \n",
      "#christmas      countdown                pubs      pies  greatbritishchefs  \n",
      "#cricket          ganguly              shoaib     pitch              ashes  \n",
      "#scubadive     freediving           sdidivers    paditv    underwaterworld  \n",
      "#brexit        stopbrexit           political  politics               ukip  \n",
      "\n",
      "[6 rows x 25 columns]\n",
      "Checkpoint 4 completed in  1.8969120979309082\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#TF-IDF Section\n",
    "start = time.time();                    \n",
    "\n",
    "#Bag of words\n",
    "#1 creation\n",
    "for txt in HT.values():\n",
    "    bow = set(bow).union(set(txt.split(' ')))\n",
    "#2 Bag of words initialization \n",
    "bowDict = {}\n",
    "for h in HT.keys():\n",
    "    bowDict[h] = dict.fromkeys(bow, 0)\n",
    "    #3 Bow updation for each class(hashtag)\n",
    "    for w in HT[h].split(' ') :\n",
    "        bowDict[h][w] += 1\n",
    "\n",
    "#Compute Term-Frequency\n",
    "def computeTF(bowDict, tweet):\n",
    "    tfDict = {}\n",
    "    tweetlen = len(tweet.split(' '))\n",
    "    for word, count in bowDict.items():\n",
    "        tfDict[word] = count/float(tweetlen)\n",
    "    return tfDict\n",
    "\n",
    "tf = {}\n",
    "for h, t in HT.items():\n",
    "    tf[h] = computeTF(bowDict[h], t)\n",
    "\n",
    "#Compute Inverse document frequency\n",
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    idfDict = HT.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList: #Each class(hashtag)\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                if word not in idfDict.keys():\n",
    "                    idfDict[word] = 1\n",
    "                else :\n",
    "                    idfDict[word] += 1\n",
    "\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "\n",
    "    return idfDict\n",
    "\n",
    "idfs = computeIDF([bowDict[h] for h in bowDict.keys()])\n",
    "\n",
    "#Compute TF-IDF\n",
    "def computeTFIDF(tfhashtxt, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfhashtxt.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "tfidfBow = {}; topfivewords = {}\n",
    "for hashtag in HT.keys():\n",
    "    tfidfBow[hashtag] = computeTFIDF(tf[hashtag], idfs)\n",
    "    topfivewords[hashtag] = dict(Counter(tfidfBow[hashtag]).most_common(5))\n",
    "\n",
    "tfidf = {}\n",
    "for hashtag, rankwords in topfivewords.items():\n",
    "    tfidf[hashtag] = rankwords.keys()\n",
    "        \n",
    "tfidfpd = pd.DataFrame.from_dict(tfidf, orient='index');\n",
    "print(tfidfpd)\n",
    "\n",
    "print(\"Checkpoint 4 completed in \", time.time() - start);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data created 65.892160654068\n",
      "Test data created 23.95490336418152\n",
      "Checkpoint 5 completed in  91.34921073913574\n",
      "Vector set created --> Success!!!\n"
     ]
    }
   ],
   "source": [
    "#Vectors set\n",
    "start = time.time();         \n",
    "\n",
    "\"\"\"To transform train and test dataset into input and output vectors \n",
    "for the multiclass-logistic regression.\"\"\"\n",
    "\n",
    "rank = tfidfpd; \n",
    "trainXY = pd.DataFrame.from_dict(trainHT, orient='index')\n",
    "testXY = pd.DataFrame.from_dict(testHT, orient='index')\n",
    "\n",
    "bow = set()\n",
    "for i in range(0, len(rank)):\n",
    "    rankwords = rank.iloc[i]\n",
    "    bow = set(bow).union(set(rankwords))\n",
    "\n",
    "#Train data X Y\n",
    "start_time = time.time()\n",
    "X = list(bow); Y = list(rank.index.get_values());\n",
    "r, c = trainXY.shape;\n",
    "train = {}; train[tuple([0]*len(X))] = 0;\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(r):\n",
    "    outY = i\n",
    "\n",
    "    for j in range(c):\n",
    "        if trainXY.iloc[i, j] != None :\n",
    "            inX = [0]*len(X)\n",
    "\n",
    "            for feature in X:\n",
    "                if feature in trainXY.iloc[i, j]:\n",
    "                    inX[X.index(feature)] = 1\n",
    "                    if tuple(inX) not in train:\n",
    "                        train[tuple(inX)] = outY\n",
    "        else : \n",
    "            break    \n",
    "\n",
    "trainpd = pd.DataFrame.from_dict(train, orient='index')\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print('Train data created',elapsed_time)\n",
    "\n",
    "#Test data X Y\n",
    "X = list(bow); Y = list(rank.index.get_values());\n",
    "r, c = testXY.shape;\n",
    "test = {}; test[tuple([0]*len(X))] = 0;\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(r):\n",
    "    outY = i\n",
    "\n",
    "    for j in range(c):\n",
    "        if testXY.iloc[i, j] != None :\n",
    "            inX = [0]*len(X)\n",
    "\n",
    "            for feature in X:\n",
    "                if feature in testXY.iloc[i, j]:\n",
    "                    inX[X.index(feature)] = 1\n",
    "                    if tuple(inX) not in test:\n",
    "                        test[tuple(inX)] = outY\n",
    "        else : \n",
    "            break  \n",
    "                \n",
    "testpd = pd.DataFrame.from_dict(test, orient='index')\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print('Test data created',elapsed_time)       \n",
    "\n",
    "print(\"Checkpoint 5 completed in \", time.time() - start); print(\"Vector set created --> Success!!!\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 481    9    0    8    0    1]\n",
      " [   3  105    5   16    0   12]\n",
      " [   3   12   57    2    0    3]\n",
      " [   7    9    0  555    3    1]\n",
      " [   0    0    0    4   47    0]\n",
      " [   3   19    5    0    0 1669]]\n",
      "Accuracy:  0.9588680487002303\n",
      "precision: [0.96780684 0.68181818 0.85074627 0.94871795 0.94       0.98991696]\n",
      "recall: [0.96392786 0.74468085 0.74025974 0.96521739 0.92156863 0.98408019]\n",
      "fscore: [0.96586345 0.71186441 0.79166667 0.95689655 0.93069307 0.98698995]\n",
      "support: [ 499  141   77  575   51 1696]\n",
      "Checkpoint 6 completed in  1.6566684246063232\n",
      "Task complete -- > Success!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files (x86)\\python\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "#Logistic regression\n",
    "start = time.time();  \n",
    "\n",
    "#Train data\n",
    "train = trainpd;\n",
    "trainX = np.array(list(train.index.get_values()))\n",
    "trainY = []\n",
    "for j in range(len(train)):\n",
    "    trainY.append(list(train.iloc[j])[0])\n",
    "trainY = np.array(trainY)\n",
    "\n",
    "#Test data\n",
    "test = testpd;\n",
    "testX = np.array(list(test.index.get_values()))\n",
    "testY = []\n",
    "for j in range(len(test)):\n",
    "    testY.append(list(test.iloc[j])[0])\n",
    "testY = np.array(testY)\n",
    "\n",
    "#Logistic regression.\n",
    "logreg = LogisticRegression( C=1e1, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg.fit(trainX, trainY)\n",
    "PredY = logreg.predict(testX)\n",
    "\n",
    "print(confusion_matrix(testY, PredY))\n",
    "\n",
    "accuracy = accuracy_score(testY, PredY)\n",
    "print('Accuracy: ', accuracy)\n",
    "\n",
    "precision, recall, fscore, support = score(testY, PredY)\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "\n",
    "print(\"Checkpoint 6 completed in \", time.time() - start);  print(\"Task complete -- > Success!!!\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
