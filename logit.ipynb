{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Successfully written.\n"
     ]
    }
   ],
   "source": [
    "#import csv\n",
    "import re\n",
    "import codecs\n",
    "#import unidecode\n",
    "#import string\n",
    "from nltk import FreqDist\n",
    "\n",
    "File = \"output_got.csv\"\n",
    "clean_data_file = \"clean_data.txt\"\n",
    "clean_data = []; all_hashtags =[];\n",
    "\n",
    "with codecs.open(clean_data_file, \"w\", \"utf-8\") as out_data:\n",
    "    with open(File, \"r\", encoding = \"utf8\") as my_input_file:\n",
    "        next(my_input_file)\n",
    "        for line in my_input_file:\n",
    "            # clean data\n",
    "            line = re.sub(\"`|@|,|~|\\n|'[ '|'] '|'?'|<.*>|<|!|\\.|@|\\$|\\*|:|%|\\+|…|\\\\\\\\|\\/|«|»|···|\\||\\•|\\?|\\(|\\)|=|-|&|;|\\_|—|~|¯|\\{|\\}|\\[|\\]|£|€|¥|¿|–|\\“|\\”|\\‘|\\’|\\\"|,|'\", \"\", line)\n",
    "            line = re.sub(\"http[a-zA-Z0-9]+\", \"\", line)\n",
    "            line = line.lower()\n",
    "            line = re.sub(\"pictwitter\",\" pictwitter\", line);\n",
    "            line = re.sub(\" # \",\" #\", line); line = re.sub(\"#\",\" #\", line); line = re.sub(\"# \",\"\", line); \n",
    "            for word in line.split(' '):\n",
    "                if word.__contains__(\"pictwitter\"): line.replace(word, '');\n",
    "                if (word == '') or (len(word) < 2) : line.replace(word, '');\n",
    "            line = re.sub(\"  \",\" \", line); line = line[:-2]\n",
    "            clean_data.append(line)\n",
    "            #line = unidecode.unidecode(line)\n",
    "            out_data.write(line+'\\n')\n",
    "\n",
    "for line in clean_data:\n",
    "    for word in line.split(' '):\n",
    "        if len(word)> 1 :\n",
    "            if word[0] == '#': all_hashtags.append(word);\n",
    "\n",
    "hashtag_file = \"hashtags.txt\"\n",
    "top_hashtags = FreqDist(all_hashtags).most_common(6)\n",
    "tweets_file = \"tweets.txt\"\n",
    "hashtags = [x[0] for x in top_hashtags]; hashtags = hashtags[1:]\n",
    "\n",
    "with codecs.open(hashtag_file, \"w\", \"utf-8\") as hashfile:\n",
    "    for tag in hashtags:\n",
    "        hashfile.write(tag + '\\n')\n",
    "\n",
    "\n",
    "with codecs.open(tweets_file, \"w\", \"utf-8\") as tweets:\n",
    "    for line in clean_data:\n",
    "        if any(s in line for s in hashtags):\n",
    "            tweets.write(line+'\\n')\n",
    "\n",
    "print('File Successfully written.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "#import string\n",
    "import math\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "#from spacy.lemmatizer import Lemmatizer as lemma\n",
    "#from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "#import pickle\n",
    "#import random\n",
    "#import time\n",
    "import numpy as np\n",
    "\n",
    "htags_file = \"hashtags.txt\"\n",
    "tweets_file = \"tweets.txt\"\n",
    "\n",
    "hashtags = []; htags_tweets = {}\n",
    "with codecs.open(htags_file, \"r\", \"utf-8\") as hashtagsFile:\n",
    "    for word in hashtagsFile:\n",
    "        tag = word.split('\\n')[0]\n",
    "        hashtags.append(tag);\n",
    "\n",
    "# Dict = {Hashtag: Tweets}, stop words removal and bag of words\n",
    "htags_tweets = dict.fromkeys(hashtags,'')\n",
    "stop_words = set(stopwords.words('english'));\n",
    "bow = set(); hashtweets = defaultdict(list); testHT = defaultdict(list);\n",
    "\n",
    "with codecs.open(tweets_file, \"r\", \"utf-8\") as tweets:\n",
    "    for tweet in tweets:\n",
    "        #Stop words removal\n",
    "        word_list = re.sub(\"`|@|,|~|\\n|'[ '|'] '|'?'\", \" \", tweet).split(' ');\n",
    "        filt_word_list = [word for word in word_list if (word not in stop_words) and (word != '' and len(word)>2) and (not word.__contains__('pictwittercom'))]\n",
    "        filtered_tweet = ' '.join(filt_word_list)\n",
    "\n",
    "        for tag in hashtags:\n",
    "            if tag in filtered_tweet:\n",
    "                flag = np.random.choice([0, 1], 1, p=[0.75, 0.25])\n",
    "                if flag == 1:\n",
    "                    #Test data\n",
    "                    testHT[tag].append(filtered_tweet)\n",
    "                else: \n",
    "                    # Train set: Dict = {Hashtag: Tweets}\n",
    "                    hashtweets[tag].append(filtered_tweet) # {Hashtag: [T1, T2, T3]}\n",
    "                    htags_tweets[tag] += re.sub('#' , '', filtered_tweet)\n",
    "\n",
    "htpanda = pd.DataFrame.from_dict(hashtweets, orient='index')\n",
    "testpanda = pd.DataFrame.from_dict(testHT, orient='index')\n",
    "\n",
    "file_name = \"trainHT.pkl\"\n",
    "htpanda.to_pickle(file_name)\n",
    "\n",
    "test_file = \"testHT.pkl\"\n",
    "testpanda.to_pickle(test_file)\n",
    "\n",
    "for txt in htags_tweets.values():\n",
    "    #bag of words: set creation\n",
    "    bow = set(bow).union(set(txt.split(' ')))\n",
    "\n",
    "#Bag of words initialization and updation for each class(hashtag)\n",
    "bowDict = {}\n",
    "for h in htags_tweets.keys():\n",
    "    bowDict[h] = dict.fromkeys(bow, 0)\n",
    "\n",
    "    for w in htags_tweets[h].split(' ') :\n",
    "        bowDict[h][w] += 1\n",
    "\n",
    "#Compute Term-Frequency\n",
    "def computeTF(bowDict, tweet):\n",
    "    tfDict = {}\n",
    "    tweetlen = len(tweet.split(' '))\n",
    "    for word, count in bowDict.items():\n",
    "        tfDict[word] = count/float(tweetlen)\n",
    "    return tfDict\n",
    "\n",
    "for h, t in htags_tweets.items():\n",
    "    tftweets = {}\n",
    "    tftweets[h] = computeTF(bowDict[h], t)\n",
    "\n",
    "#Compute Inverse document frequency\n",
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    idfDict = htags_tweets.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList: #Each class(hashtag)\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                if word not in idfDict.keys():\n",
    "                    idfDict[word] = 1\n",
    "                else :\n",
    "                    idfDict[word] += 1\n",
    "\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "\n",
    "    return idfDict\n",
    "\n",
    "idfs = computeIDF([bowDict[h] for h in bowDict.keys()])\n",
    "\n",
    "#Compute TF-IDF\n",
    "def computeTFIDF(tfhashtxt, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfhashtxt.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "tfidfBow = {}; topfivewords = {}\n",
    "for hashtag in htags_tweets.keys():\n",
    "    tfidfBow[hashtag] = computeTFIDF(bowDict[hashtag], idfs)\n",
    "    topfivewords[hashtag] = dict(Counter(tfidfBow[hashtag]).most_common(40))\n",
    "\n",
    "tfidf_file = \"tfidf.txt\"\n",
    "tfidf = {}\n",
    "\n",
    "with codecs.open(tfidf_file, \"w\", \"utf-8\") as out_data:\n",
    "    for ht, rw in topfivewords.items():\n",
    "        out_data.write(ht+ ' '+str(' '.join(rw.keys()) + '\\n'))\n",
    "        tfidf[ht] = rw.keys()\n",
    "\n",
    "tfidfpd = pd.DataFrame.from_dict(tfidf, orient='index')\n",
    "\n",
    "file_name = \"tfidf.pkl\"\n",
    "tfidfpd.to_pickle(file_name)\n",
    "\n",
    "print(\"Success!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\"\"\"To transform train and test dataset into input and output vectors \n",
    "for the multiclass-logistic regression.\"\"\"\n",
    "\n",
    "rank = pickle.load(open('tfidf.pkl', 'rb'));\n",
    "trainXY = pickle.load(open('trainHT.pkl', 'rb'));\\\n",
    "\n",
    "bow = set()\n",
    "for i in range(0, len(rank)):\n",
    "    rankwords = rank.iloc[i]\n",
    "    bow = set(bow).union(set(rankwords))\n",
    "\n",
    "#Train data X Y\n",
    "X = list(bow); Y = list(rank.index.get_values());\n",
    "r, c = trainXY.shape;\n",
    "train = {}; train[tuple([0]*len(X))] = 0;\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(r):\n",
    "    outY = i\n",
    "\n",
    "    for j in range(c):\n",
    "        if trainXY.ix[i, j] != None :\n",
    "            inX = [0]*len(X)\n",
    "\n",
    "            for feature in X:\n",
    "                if feature in trainXY.ix[i, j]:\n",
    "                    inX[X.index(feature)] = 1\n",
    "                    if tuple(inX) not in train:\n",
    "                        train[tuple(inX)] = outY\n",
    "        else : \n",
    "            break    \n",
    "\n",
    "trainpd = pd.DataFrame.from_dict(train, orient='index')\n",
    "trainpd.to_pickle(\"trainXY.pkl\")  \n",
    "elapsed_time = time.time() - start_time\n",
    "print('Train data created',elapsed_time)\n",
    "\n",
    "#Test data X Y\n",
    "rank = pickle.load(open('tfidf.pkl', 'rb'));\n",
    "bow = set()\n",
    "\n",
    "for i in range(0, len(rank)):\n",
    "    rankwords = rank.iloc[i]\n",
    "    bow = set(bow).union(set(rankwords))\n",
    "    \n",
    "testXY = pickle.load(open('testHT.pkl', 'rb'));\n",
    "X = list(bow); Y = list(rank.index.get_values());\n",
    "r, c = testXY.shape;\n",
    "test = {}; test[tuple([0]*len(X))] = 0;\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(r):\n",
    "    outY = i\n",
    "\n",
    "    for j in range(c):\n",
    "        if testXY.ix[i, j] != None :\n",
    "            inX = [0]*len(X)\n",
    "\n",
    "            for feature in X:\n",
    "                if feature in testXY.ix[i, j]:\n",
    "                    inX[X.index(feature)] = 1\n",
    "                    if tuple(inX) not in test:\n",
    "                        test[tuple(inX)] = outY\n",
    "        else : \n",
    "            break  \n",
    "                \n",
    "testpd = pd.DataFrame.from_dict(test, orient='index')\n",
    "testpd.to_pickle(\"testXY.pkl\") \n",
    "elapsed_time = time.time() - start_time\n",
    "print('Test data created',elapsed_time)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import pickle\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "#Train data\n",
    "train = pickle.load(open('trainXY.pkl', 'rb'));# print(rank);\n",
    "trainX = np.array(list(train.index.get_values()))\n",
    "\n",
    "trainY = []\n",
    "for j in range(len(train)):\n",
    "    trainY.append(list(train.ix[j])[0])\n",
    "trainY = np.array(trainY)\n",
    "\n",
    "#Test data\n",
    "test = pickle.load(open('testXY.pkl', 'rb'));# print(rank);\n",
    "testX = np.array(list(test.index.get_values()))\n",
    "\n",
    "testY = []\n",
    "for j in range(len(test)):\n",
    "    testY.append(list(test.ix[j])[0])\n",
    "testY = np.array(testY)\n",
    "\n",
    "#Logistic regression.\n",
    "logreg = LogisticRegression( C=1e1, solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg.fit(trainX, trainY)\n",
    "PredY = logreg.predict(testX)\n",
    "\n",
    "print(confusion_matrix(testY, PredY))\n",
    "accuracy = accuracy_score(testY, PredY)\n",
    "print('Accuracy: ', accuracy)\n",
    "\n",
    "\"\"\"\n",
    "precision, recall, fscore, support = score(testY, PredY)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support)) \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
